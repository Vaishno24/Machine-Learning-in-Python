{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASSUMPTION:\n",
    "All the features in the given dataset are independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funtion to classify data for better understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataClassification(dataset):\n",
    "    \n",
    "    # To get the no of observations or datapoints in the given data\n",
    "    C = dataset.iloc[:, 10]\n",
    "    B = []\n",
    "    M = []\n",
    "     \n",
    "    # For every class label \"2\" this loop will append \"B\" to an empty list and for every class label \"4\" it will append \"M\"    \n",
    "    for i in range(0, len(C)):\n",
    "        if C[i] == 2:\n",
    "            B.append(\"B\")\n",
    "            l1 = len(B)\n",
    "        \n",
    "        elif C[i] == 4:\n",
    "            M.append(\"M\")\n",
    "            l2 = len(M)\n",
    "    test_patients = dataset.shape[0]\n",
    "    \n",
    "    # To print our basic understanding of the dataset \n",
    "    a = str(\"The data consists of \" + str(test_patients) + \" different diagnosis in which \" + str(l1) \n",
    "            + \" are Benign and \" + str(l2) + \" are Malignant\")\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to split the dataset into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingDataset(dataset, splitRatio):\n",
    "    \n",
    "    total_data = dataset.shape[0]\n",
    "    \n",
    "    # Splitting the data into two groups for training and testing and this data is for training\n",
    "    # Also removing the sample code number feature as it has no affect on class labels\n",
    "    train_data = dataset.iloc[0:(int(splitRatio*total_data)), 1:11]\n",
    "    \n",
    "    # Converting the data into a dataframe\n",
    "    d = pd.DataFrame(train_data)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testingData(dataset, splitRatio):\n",
    "    \n",
    "    total_data = dataset.shape[0]\n",
    "    \n",
    "    # This data is for testing\n",
    "    # Also removing the sample code number feature as it has no affect on class labels\n",
    "    test_data = dataset.iloc[int(total_data - total_data*(1 - splitRatio)):total_data, 1:11]\n",
    "    \n",
    "    # Converting the data into dataframe\n",
    "    d1 = pd.DataFrame(test_data)\n",
    "    return d1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to understand the how the split happened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataDivision(data1, data2):\n",
    "     \n",
    "    # To analyze the how the data is splitted \n",
    "    lt1 = len(data1)\n",
    "    lt2 = len(data2)\n",
    "    \n",
    "    # To print our basic understanding of the splitting process of the data \n",
    "    b = str(\"The data is divided into \" + str(lt1) + \" training data samples and \" + str(lt2) + \" testing data samples\") \n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testdataclassification(test_data):\n",
    "    \n",
    "    Bdata, Mdata =  (test_data['Class'].value_counts())\n",
    "    testshape = test_data.shape[0]\n",
    "    \n",
    "    l1 = str(\"The testing data consists of \" + str(testshape) + \" different diagnosis in which \" + str(Bdata) + \n",
    "          \" are Benign and \" + str(Mdata) + \" are Malignant\")\n",
    "    return l1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to group data based on class label to compute the individual means and standard deviations of the independent features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouping(dataset):\n",
    "    \n",
    "    # Grouping of data is necessary in a Naive bayes classifier as we have to calculate the individual probability with respect\n",
    "    # to a feature in a particular class labelled label \n",
    "    grouped = dataset.groupby('Class')\n",
    "    B_data = grouped.get_group(2)\n",
    "    B_data = B_data.iloc[:, 0:9]\n",
    "    \n",
    "    M_data = grouped.get_group(4)\n",
    "    M_data = M_data.iloc[:, 0:9]\n",
    "    \n",
    "    return B_data, M_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(points):\n",
    "    \n",
    "    # The mean is defined as the sum of the datapoints or observations divided by the number observations or datapoints\n",
    "    return np.sum(points)/np.size(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MeanSet(data1,data2):\n",
    "    \n",
    "    B_mean_set = []\n",
    "    M_mean_set = []\n",
    "    \n",
    "    # To calculate the mean of every independent feature for both classes\n",
    "    for i in range(0,len(data1.columns)):\n",
    "        p = data1.iloc[:,i]\n",
    "        m = mean(p)\n",
    "        B_mean_set.append(m)\n",
    "        \n",
    "    for i in range(0,len(data2.columns)):\n",
    "        p = data2.iloc[:,i]\n",
    "        m = mean(p)\n",
    "        M_mean_set.append(m)\n",
    "        \n",
    "    return B_mean_set, M_mean_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard deviation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def st_dev(points, mean):\n",
    "    \n",
    "   # Standard deviation is defined as the square root of the sum of difference between the observations and their mean \n",
    "   # divided by the no.0f observations\n",
    "   variance = np.sum([(x - mean)**2 for x in points])/(np.size(points) - 1)\n",
    "   return np.sqrt(variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SDset(data1, data2, mean1, mean2):\n",
    "    \n",
    "    B_sd_set = []\n",
    "    M_sd_set = []\n",
    "        \n",
    "    # To calculate the standard deviation of every independent feature for both classes\n",
    "    for i in range(0, len(data1.columns)):\n",
    "        p = data1.iloc[:,i]\n",
    "        sd = st_dev(p, mean1[i])\n",
    "        B_sd_set.append(sd)\n",
    "    \n",
    "    for i in range(0, len(data2.columns)):\n",
    "        p = data2.iloc[:,i]\n",
    "        sd = st_dev(p, mean2[i])\n",
    "        M_sd_set.append(sd)\n",
    "    \n",
    "    return B_sd_set, M_sd_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Guassian probability density function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "\n",
    "def probability(x, mean, sd):\n",
    "    \n",
    "    # This is the gaussian probability density function, which has the mathematical equation\n",
    "    # (1/sqrt(2*pi*(sd)^2)) * exp((-sum(xi-mean)^2)/2*(sd^2))\n",
    "    e = np.exp(-((x - mean)**2 / (2 * sd**2 )))\n",
    "    return (1 / (np.sqrt(2 * pi) * sd)) * e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to predict class labels for the given testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(testData, mean1, mean2, sd1, sd2):\n",
    "    \n",
    "    test_prob_set1 = []\n",
    "    test_prob_set2 = []\n",
    "    p1 = []\n",
    "    p2 = []\n",
    "    \n",
    "    # To predict the class labels for testing data\n",
    "    # We took two empty lists as we have two classes and we need to calculate the probabilities of a datapoint with two \n",
    "    # groups of data\n",
    "    for i in range(0, len(testData.index)):\n",
    "        for j in range(0, len(testData.columns)-1):\n",
    "            prob1 = probability(testData.iloc[i,j], mean1[j], sd1[j])\n",
    "            prob2 = probability(testData.iloc[i,j], mean2[j], sd2[j])\n",
    "            test_prob_set1.append(prob1)\n",
    "            test_prob_set2.append(prob2) \n",
    "            \n",
    "        # Multiply the independent probabilities to get the probability for that particular group of data\n",
    "        a = np.prod(test_prob_set1)\n",
    "        b = np.prod(test_prob_set2)\n",
    "        \n",
    "        # Then append the two prbabilities in two empty lists\n",
    "        p1.append(a)\n",
    "        p2.append(b)\n",
    "    \n",
    "    return p1, p2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report metrics consists of Accuracy, Precision, Recall, F1-score and Confusion matrix to evaluate the preformance of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classification_report(testData, p, q):\n",
    "    \n",
    "    #Intialize the true positives, false positives, true negatives and the false negatives\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "\n",
    "    # Iterate through the p and q lists comparing the predicted labels with the actual ones and computing \n",
    "    # the TP, TN, FP and FN\n",
    "    for i in range(0, len(testData.index)):\n",
    "        if (p[i] == max(p[i], q[i]) and testData.iloc[i,9] == 2):\n",
    "            TP += 1\n",
    "        elif (q[i] == max(p[i], q[i]) and testData.iloc[i,9] == 4):\n",
    "            TN += 1\n",
    "        elif (q[i] == max(p[i], q[i]) and testData.iloc[i,9] == 2):   \n",
    "            FN += 1\n",
    "        elif (p[i] == max(p[i], q[i]) and testData.iloc[i,9] == 4):\n",
    "            FP += 1\n",
    "            \n",
    "    # Accuracy is the measure of many correct predictions the model made        \n",
    "    A = (TP + TN)/(TP + TN + FP + FN) * 100.0\n",
    "    \n",
    "    # precision expresses the proportion of the data points our model says was relevant actually were relevant.\n",
    "    P1 = (TP)/(TP + FP) * 100.0\n",
    "    P2 = (TN)/(TN + FN) * 100.0\n",
    "    \n",
    "    # Recall expresses the ability to find all relevant instances in a dataset\n",
    "    R1 = (TP)/(TP + FN) * 100.0\n",
    "    R2 = (TN)/(TN + FP) * 100.0\n",
    "    \n",
    "    # Harmonic mean of recall and precision \n",
    "    F1 = (2*P1*R1)/(R1 + P1)\n",
    "    F2 = (2*P2*R2)/(P2 + R2)\n",
    "    \n",
    "    # A matrix to represent all types of predictions \n",
    "    CM = np.array([[TP, FP],[FN, TN]])\n",
    "    \n",
    "    return A, P1, P2, R1, R2, F1, F2, CM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A main() function to simplify the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # Reading the data file and converting it into a dataframe\n",
    "    data = pd.read_excel(r\"C:\\Users\\sriva\\OneDrive\\Desktop\\Python_for_ML_project_dataset\\breast-cancer-wisconsin.xlsx\")\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Processing the data\n",
    "    m = dataClassification(data)\n",
    "    splitRatio = 0.75\n",
    "    df1 = trainingDataset(data, splitRatio)\n",
    "    df2 = testingData(data, splitRatio)\n",
    "    l = DataDivision(df1, df2)\n",
    "    t = testdataclassification(df2)\n",
    "    \n",
    "    # Parameter calculation\n",
    "    B_data, M_data = grouping(df1)\n",
    "    B_mean_set, M_mean_set = MeanSet(B_data,M_data)\n",
    "    B_sd_set, M_sd_set = SDset(B_data, M_data, B_mean_set, M_mean_set)\n",
    "    \n",
    "    # Predictions\n",
    "    p1, p2 = prediction(df2, B_mean_set, M_mean_set, B_sd_set, M_sd_set)\n",
    "    \n",
    "    # Model evaluation\n",
    "    Acc, PrecisionB, PrecisionM, RecallB, RecallM, F1_scoreB, F1_scoreM, cm = Classification_report(df2, p1, p2)\n",
    "    \n",
    "    print(m)\n",
    "    print(l)\n",
    "    print(t)\n",
    "    print(\"\\n\" + \"The Accuracy of the NaiveBayes model is: \"+ str(Acc) + \"%\")\n",
    "    print(\"\\n\" + \"The Precision of the NaiveBayes model is: \"+ str((PrecisionB + PrecisionM)/2) + \"%\")\n",
    "    print(\"\\n\" + \"The Recall of the NaiveBayes model is: \"+ str((RecallB + RecallM)/2) + \"%\")\n",
    "    print(\"\\n\" + \"The F1 score of the NaiveBayes model is: \"+ str((F1_scoreB + F1_scoreM)/2) + \"%\")\n",
    "    print(\"\\n\" + \"The Confusion matrix of the NaiveBayes model is: \" + \"\\n\" + str(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data consists of 699 different diagnosis in which 458 are Benign and 241 are Malignant\n",
      "The data is divided into 524 training data samples and 175 testing data samples\n",
      "The testing data consists of 175 different diagnosis in which 137 are Benign and 38 are Malignant\n",
      "\n",
      "The Accuracy of the NaiveBayes model is: 98.28571428571429%\n",
      "\n",
      "The Precision of the NaiveBayes model is: 98.92857142857142%\n",
      "\n",
      "The Recall of the NaiveBayes model is: 96.05263157894737%\n",
      "\n",
      "The F1 score of the NaiveBayes model is: 97.40368923396468%\n",
      "\n",
      "The Confusion matrix of the NaiveBayes model is: \n",
      "[[137   3]\n",
      " [  0  35]]\n",
      "\n",
      "The runtime of the Naive Bayes model is 0.20969414710998535 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# starting time\n",
    "start = time.time()\n",
    "\n",
    "main()\n",
    "\n",
    "# stoping time\n",
    "end = time.time()\n",
    "\n",
    "# Print the runtime\n",
    "print(\"\\n\" + f\"The runtime of the Naive Bayes model is {end - start}\" + \" sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
